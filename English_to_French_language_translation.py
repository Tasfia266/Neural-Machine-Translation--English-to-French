# -*- coding: utf-8 -*-
"""English_to_french_language_translation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hLGZ_PD9vwtgsI2axXafWTO93lpmDmrn
"""

from google.colab import drive 
drive.mount ('/content/drive')

import numpy as np 
import tensorflow as tf

batch_size=64 
latent_dim= 256 # number of neurons in LSTM
num_sample= 10000 # number of samples to be train on

data_path= "/content/drive/MyDrive/fra.txt"

# Prepare the data. Vectorize the data 
input_texts=[] 
target_texts=[] 
input_characters= set() 
target_characters= set() 
with open (data_path, 'r', encoding='utf-8') as f:
  lines= f.read().split('\n') # reading the text file
for line in lines [:min(num_sample, len(lines)-1)] : #iterate over each line of the text file
  input_text, target_text, _= line.split ('\t') # split the lines by tab 
  target_text= "\t"+target_text+"\n" 
  input_texts.append (input_text) # append the input (english) sentence in list
  target_texts.append (target_text) #append the input (french) sentence in list
  for char in input_text: 
    if char not in input_characters: # add the character if it is already not present in list
      input_characters.add(char) 
  for  char in target_text: 
    if char not in target_characters: 
      target_characters.add (char)

target_texts

input_characters= sorted(list(input_characters))
target_characters= sorted (list (target_characters))
num_encoder_tokens= len (input_characters) 
num_decoder_token= len (target_characters)
max_encoder_seq_length=max ([len(text) for text in input_texts])
max_decoder_seq_length= max ([len(text) for text in target_texts])

len (input_texts), num_encoder_tokens, num_decoder_token, max_encoder_seq_length, max_decoder_seq_length

# creating a dictionary for input token 
input_token_index= dict ([(char,i) for i,char in enumerate (input_characters)])
target_token_index= dict ([(char,i) for i,char in enumerate (target_characters)])

input_token_index

target_token_index

len(target_token_index)

encoder_input_data= np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')
decoder_input_data= np.zeros ((len(input_texts), max_decoder_seq_length, num_decoder_token), dtype='float32')

encoder_input_data.shape, decoder_input_data.shape

decoder_target_data= np.zeros((len (input_texts), max_decoder_seq_length, num_decoder_token), dtype='float32')

for i, input_text in enumerate(input_texts): 
  for t, char in enumerate (input_text): 
    encoder_input_data[i, t, input_token_index[char]]=1.0
  encoder_input_data[i, t+1:, input_token_index[" "]]=1.0

encoder_input_data[0]

for i, target_text in enumerate (target_texts): 
  for t, char in enumerate (target_text): 
    decoder_input_data[i,t,target_token_index[char]]=1.0
    if t>0: 
      decoder_target_data[i,t-1, target_token_index[char]]=1.0
  decoder_input_data[i, t+1:, target_token_index[' ']]=1.0 
  decoder_target_data[i, t:, target_token_index[' ']]=1.0

decoder_input_data[0]

decoder_target_data[0]

# Build the model 
import keras
encoder_inputs= keras.Input (shape= (None, num_encoder_tokens))
encoder= keras.layers.LSTM (latent_dim, return_state= True) 
encoder_outputs, state_h, state_c= encoder(encoder_inputs) 

# We discard encoder_outputs and keep the states 
encoder_states= [state_h, state_c]

decoder_inputs=keras.Input (shape= (None, num_decoder_token))
decoder= keras.layers.LSTM (latent_dim, return_sequences= True, return_state= True)
decoder_outputs,_,_= decoder (decoder_inputs, initial_state= encoder_states)
decoder_dense= keras.layers.Dense (num_decoder_token, activation='softmax') 
decoder_outputs= decoder_dense (decoder_outputs)

model= keras.Model ([encoder_inputs, decoder_inputs], decoder_outputs)

model.summary()

model.compile (optimizer='adam', loss= 'categorical_crossentropy', metrics=['accuracy'])

model.fit ([encoder_input_data, decoder_input_data], decoder_target_data, batch_size= batch_size, validation_split=0.2, epochs=300, verbose=2)

#RUN Inference

model.input[0]

model.layers[3]

encoder_input= model.input[0] #input 1
encoder_outputs, state_h_enc, state_c_enc= model.layers[2].output 
encoder_states= [state_h_enc, state_c_enc] 
encoder_model= keras.Model (encoder_input, encoder_states )

decoder_input= model.input[1] # input 1 
decoder_state_input_h= keras.Input (shape= (latent_dim,))
decoder_state_input_c= keras.Input (shape= (latent_dim, ))
decoder_state_input= [decoder_state_input_h, decoder_state_input_c] 
decoder_lstm= model.layers[3] 
decoder_outputs, state_h_dec, state_c_dec= decoder_lstm (decoder_input, initial_state= decoder_state_input)

decoder_state= [state_h_dec, state_c_dec] 
decoder_dense= model.layers[4] 
decoder_outputs= decoder_dense(decoder_outputs) 

decoder_model=keras.Model ([decoder_input]+decoder_state_input, [decoder_outputs]+decoder_state)

input_token_index.items()

reverse_input_token_index= dict((i,char) for char,i in input_token_index.items())
reverse_target_token_index= dict ((i, char) for char, i in target_token_index.items())

target_token_index

target_token_index['\t']

a=np.zeros ((1,1, num_decoder_token))
a[0,0,target_token_index['\t']]=1.0 
print (a)

def decode_seq(input_seq): 
  # Encode the input as state vector 
  states_value= encoder_model.predict (input_seq) 
 # print ('states value', states_value)
  # Generate empty target sequence of length 1 
  target_seq= np.zeros ((1,1, num_decoder_token)) 
  # populate the first character of target sequence with start character 
  target_seq[0,0,target_token_index['\t']]=1.0
  # Sampling loop for a betch of sequences 
  stop_condition= False 
  decoded_sentence="" 
  while not stop_condition: 
    output_tokens, h,c= decoder_model.predict ([target_seq]+states_value)
   # print ('output_tokens', output_tokens)
    # Sample a token 
    sample_token_index= np.argmax (output_tokens[0,-1,:])
    #print ('sample_token_index', sample_token_index) 
    sample_char= reverse_target_token_index[sample_token_index]
    #print ('sample char',sample_char)
    decoded_sentence+=sample_char 
    if sample_char=='\n' or len (decoded_sentence)>max_decoder_seq_length: 
      stop_condition= True 
    target_seq= np.zeros ((1,1,num_decoder_token)) 
    target_seq[0,0,sample_token_index]=1.0 
    #Update state 
    state_value=[h,c]  
  return decoded_sentence

def decode_sequence_2(input_seq):
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq)

    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1, 1, num_decoder_token))
    # Populate the first character of target sequence with the start character.
    target_seq[0, 0, target_token_index["\t"]] = 1.0

    # Sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1).
    stop_condition = False
    decoded_sentence = ""
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_token_index[sampled_token_index]
        decoded_sentence += sampled_char

        # Exit condition: either hit max length
        # or find stop character.
        if sampled_char == "\n" or len(decoded_sentence) > max_decoder_seq_length:
            stop_condition = True

        # Update the target sequence (of length 1).
        target_seq = np.zeros((1, 1, num_decoder_token))
        target_seq[0, 0, sampled_token_index] = 1.0

        # Update states
        states_value = [h, c]
    return decoded_sentence

for seq_index in range (15): 
  input_seq= encoder_input_data [seq_index:seq_index+1]
  decoded_sentence=decode_sequence_2 (input_seq) 
  print ('Input sequence', input_texts[seq_index]) 
  print ('decoded sentence', decoded_sentence)